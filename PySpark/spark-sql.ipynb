{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5975cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Python Spark SQL basic example')\\\n",
    "        .config('spark.some.config.option', 'some-value')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "### Create json file using spark\n",
    "# sparkContext로 객체 생성\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# json 파일 읽어들이기\n",
    "path = '/home/hadoop/prj2/people.json'\n",
    "peopleDF = spark.read.json(path)\n",
    "\n",
    "# printSchema()로 json파일의 스키마 형태 볼수 있음\n",
    "peopleDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ae961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임을 사용하는 임시의 view(가상의 테이블) 생성\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# spark에서 제공하는 sql 메소드를 이용해 쿼리 날리기\n",
    "# 쿼리문에서 people 테이블은 위에서 만들었던 view 테이블임!\n",
    "teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagerNamesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8dbbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/01 08:03:42 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/04/01 08:03:42 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/04/01 08:03:47 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "22/04/01 08:03:47 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hadoop@127.0.1.1\n",
      "22/04/01 08:03:47 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "22/04/01 08:03:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json 파일 읽어들이기\n",
    "path = '/home/hadoop/prj2/people.json'\n",
    "df = spark.read.json(path)\n",
    "\n",
    "# Global Temporary View 생성\n",
    "df.createOrReplaceGlobalTempView('people')\n",
    "\n",
    "# 'global_temp' 라는 키워드 꼭 붙여주자!\n",
    "sqlDF = spark.sql('SELECT * FROM global_temp.people')\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af69e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|[Columbus, Ohio]| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 또한 데이터프레임은 RDD[String] 자료구조를 이용해서 json 데이터셋을 데이터프레임으로 만들 수 있음\n",
    "jsonStrings = ['{\"name\": \"Yin\", \"address\":{\"city\":\"Columbus\", \"state\":\"Ohio\"}}']\n",
    "# json -> RDD형식으로 만들기\n",
    "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "# json파일 읽어오기\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f5c047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json 파일 읽어들이기\n",
    "path = '/home/hadoop/prj2/people.json'\n",
    "df = spark.read.json(path)\n",
    "\n",
    "# name 칼럼 select 해서 살펴보기\n",
    "df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3156bb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json 파일 읽어들이기\n",
    "path = '/home/hadoop/prj2/people.json'\n",
    "df = spark.read.json(path)\n",
    "\n",
    "# name 칼럼 select 해서 살펴보기\n",
    "df.select(df['name'], df['age']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4446e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json 파일 읽어들이기\n",
    "path = '/home/hadoop/prj2/people.json'\n",
    "df = spark.read.json(path)\n",
    "\n",
    "# age가 20보다 큰 데이터만 추출\n",
    "df.filter(df['age'] > 20).show()\n",
    "# age 칼럼으로 그룹핑 하고 데이터의 개수를 집계해줌\n",
    "df.groupBy('age').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059d52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
