{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMrJE2N3KxEWR4qTpNLQY3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/KOSA_BIGDATA_DEEPLEARNING/blob/main/MapReduce/mapreduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MapReduce 실습\n",
        "https://medium.com/geekculture/mapreduce-with-python-5d12a772d5b3"
      ],
      "metadata": {
        "id": "FpW6XDRXsyY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mrjob\n",
        "mrjob is the easiest route to writing Python programs that run on Hadoop. If you use mrjob, you’ll be able to test your code locally without installing Hadoop or run it on a cluster of your choice.\n",
        "\n",
        "Additionally, mrjob has extensive integration with Amazon Elastic MapReduce. Once you’re set up, it’s as easy to run your job in the cloud as it is to run it on your laptop.\n",
        "\n",
        "Here are a number of features of mrjob that make writing MapReduce jobs easier:\n",
        "\n",
        "Keep all MapReduce code for one job in a single class\n",
        "Easily upload and install code and data dependencies at runtime\n",
        "Switch input and output formats with a single line of code\n",
        "Automatically download and parse error logs for Python tracebacks\n",
        "Put command line filters before or after your Python code\n",
        "If you don’t want to be a Hadoop expert but need the computing power of MapReduce, mrjob might be just the thing for you."
      ],
      "metadata": {
        "id": "bFv4N6fEtC1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3CP556Bsx17",
        "outputId": "a07ce55f-6d91-4795-c69c-581bf8bea215"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 184 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 194 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 215 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 225 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 235 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 256 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 266 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 276 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 286 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 296 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 307 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 317 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 327 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 337 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 348 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 358 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 368 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 378 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 389 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 399 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 409 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 419 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 430 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 439 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from mrjob) (3.13)\n",
            "Installing collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mrjob.readthedocs.io/en/latest/guides/quickstart.html"
      ],
      "metadata": {
        "id": "2NOlTdBxtmef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "\n",
        "\n",
        "class MRWordFrequencyCount(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        yield \"chars\", len(line)\n",
        "        yield \"words\", len(line.split())\n",
        "        yield \"lines\", 1\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRWordFrequencyCount.run()"
      ],
      "metadata": {
        "id": "dxkvGp_9to-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python mr_word_count.py my_file.txt"
      ],
      "metadata": {
        "id": "JiShdWkZtvCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx2Ekt3LsfRN"
      },
      "outputs": [],
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "#split by ,\n",
        "columns = 'Review,Rating'.split(',')\n",
        "class NoRatings(MRJob):\n",
        "   def steps(self):\n",
        "        return[\n",
        "            MRStep(mapper=self.mapper_get_ratings,\n",
        "                  reducer=self.reducer_count_ratings)\n",
        "        ]\n",
        "#Mapper function \n",
        "    def mapper_get_ratings(self, _, line):\n",
        "       reader = csv.reader([line])\n",
        "       for row in reader:\n",
        "           zipped=zip(columns,row)\n",
        "           diction=dict(zipped)\n",
        "           ratings=diction['Rating']\n",
        "           #outputing as key value pairs\n",
        "           yield ratings, 1\n",
        "#Reducer function\n",
        "   def reducer_count_ratings(self, key, values):\n",
        "       yield key, sum(values)\n",
        "if __name__ == \"__main__\":\n",
        "    NoRatings.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #run locallly \n",
        "# #Hotel_Reviews.csv is the dataset.\n",
        "# #verify whether the both dataset(Hotel_Reviews.csv) and the python file(NoRatings.py) are there in current directory. uing 'ls' command.\n",
        "# python NoRatings.py Hotel_Reviews.csv\n",
        "# #run with hadoop\n",
        "# #python [python file] -r hadoop --hadoop-streaming-jar [The_path_of_Hadoop_Streaming_jar] [dataset]\n",
        "# python NoRatings.py -r hadoop --hadoop-streaming-jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.1 jar Hotel_Reviews.csv"
      ],
      "metadata": {
        "id": "xJxF30MouPQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DwDkrDkvvomF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}