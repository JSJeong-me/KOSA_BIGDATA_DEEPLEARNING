{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/KOSA_BIGDATA_DEEPLEARNING/blob/main/GPT4All_Lora_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mnf7LdkRdGC",
        "outputId": "5f39ddbf-c620-46b0-8dff-67cdcb3727fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install datasets loralib sentencepiece\n",
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import textwrap\n",
        "\n",
        "peft_model_id = \"nomic-ai/gpt4all-lora\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             return_dict=True,\n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map='auto')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "vH2-YRnRSN98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def gpt4all_generate(text):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "\n",
        "    print(\"Generating...\")\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        # return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "\n",
        "    wrapped_text = textwrap.fill(tokenizer.decode(generation_output[0]), width=100)\n",
        "    print(wrapped_text)\n"
      ],
      "metadata": {
        "id": "7DgABBNkSrGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "What are the differences between alpacas and sheep?\n",
        "### Response:\"\"\"\n",
        "\n",
        "gpt4all_generate(PROMPT)"
      ],
      "metadata": {
        "id": "TyxQrFAFS7Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "Explain what a rainbow is.\n",
        "### Response:\"\"\"\n",
        "gpt4all_generate(PROMPT)"
      ],
      "metadata": {
        "id": "oBC_ht3j3-wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\\n\n",
        "### Instruction:\n",
        "Create a list of things I need to do to plan for my wife's birthday party.\n",
        "### Response:\"\"\"\n",
        "gpt4all_generate(PROMPT)"
      ],
      "metadata": {
        "id": "wbnSODCG3--4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = '''Pretend you are smart chatbot that is friendly but drunk, answer the question asked of you:\n",
        "\\n\n",
        "###\n",
        "question:\n",
        "Write me an email to Sam Altman explaining that GPT-4 should be open source.\n",
        "\\n\n",
        "###\n",
        "response:\n",
        "'''\n",
        "\n",
        "gpt4all_generate(PROMPT)"
      ],
      "metadata": {
        "id": "Oynj0fXRV2kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\\n\n",
        "### Instruction:\n",
        "Write a rhyming limerick about a cat called Max.\n",
        "\\n\n",
        "### Response:\"\"\"\n",
        "gpt4all_generate(PROMPT)"
      ],
      "metadata": {
        "id": "QUFP6lj7-Q3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatGPT & GPT-4 - limericks about Max\n",
        "\n",
        "-1\n",
        "\n",
        "Max was a feline of pride,  \n",
        "Never one to hide,  \n",
        "He strutted with grace,  \n",
        "And owned every space,  \n",
        "A true king of his domain, he'd decide.  \n",
        "\n",
        "-2\n",
        "\n",
        "Max loved to perch up high,  \n",
        "Where he could watch the world go by,  \n",
        "From his lofty view,  \n",
        "He'd contemplate what to do,  \n",
        "And sometimes let out a contented sigh.  \n",
        "\n",
        "-3 GPT-4\n",
        "\n",
        "There once was a cat named Max  \n",
        "Whose paws had incredible attacks  \n",
        "He'd leap and he'd pounce  \n",
        "On a mouse in the house  \n",
        "And no prey could ever relax  "
      ],
      "metadata": {
        "id": "b0tA8VVjGcJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\\n\n",
        "### Instruction:\n",
        "Write a python function the detects if a number is a prime number or not.\n",
        "\\n\n",
        "### Response:\"\"\"\n",
        "gpt4all_generate(PROMPT)"
      ],
      "metadata": {
        "id": "bfiw3FrRV9ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_prime(num):\n",
        "    for i in range(2, num + 1):\n",
        "        if (num % i) == 0:\n",
        "            return False\n",
        "        else:\n",
        "            return True"
      ],
      "metadata": {
        "id": "uDFQWKsODCGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_prime(15) #"
      ],
      "metadata": {
        "id": "NhDZy28_Fmjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwWKYm86FrAW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}