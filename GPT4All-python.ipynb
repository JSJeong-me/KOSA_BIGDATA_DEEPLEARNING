{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4Q81n0RMYiK1sUcwRdSbV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/KOSA_BIGDATA_DEEPLEARNING/blob/main/GPT4All-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wame838rlxZY",
        "outputId": "1b97c985-985d-4e36-ab2b-f8e34ba0097b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-1.0.8-py3-none-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2023.7.22)\n",
            "Installing collected packages: gpt4all\n",
            "Successfully installed gpt4all-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install gpt4all"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
        "output = model.generate(\"The capital of France is \", max_tokens=1)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNisqxrpnDa9",
        "outputId": "cef38a1e-13ab-4240-ca1e-83f98982f890"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT4All(model_name='orca-mini-3b.ggmlv3.q4_0.bin')\n",
        "with model.chat_session():\n",
        "    response1 = model.generate(prompt='hello', temp=0)\n",
        "    response2 = model.generate(prompt='write me a short poem', temp=0)\n",
        "    response3 = model.generate(prompt='thank you', temp=0)\n",
        "    print(model.current_chat_session)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk0E9uG9m4As",
        "outputId": "530168b3-3979-4c70-b345-af866156507e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
            "[{'role': 'system', 'content': '### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': ' Hello! How may I assist you today?'}, {'role': 'user', 'content': 'write me a short poem'}, {'role': 'assistant', 'content': \" Sure, here's a short poem for you:\\nBeneath the autumn sky so blue,\\nThe leaves fall down like rain.\\nA rustling carpet of gold and red,\\nAs nature's beauty takes its fill.\\nThe air is crisp and cool and clean,\\nAnd the world around us grows.\"}, {'role': 'user', 'content': 'thank you'}, {'role': 'assistant', 'content': \" You're welcome! Is there anything else I can help you with?\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
        "tokens = []\n",
        "for token in model.generate(\"The capital of France is\", max_tokens=20, streaming=True):\n",
        "    tokens.append(token)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Ssx0d7mvaF",
        "outputId": "110c231a-6b1f-49a7-ea3d-1321a1ac7856"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
            "[' Paris', ',', ' is', ' the', ' largest', ' city', ' in', ' the', ' country', ' and', ' is', ' located', ' in', ' the', ' northern', ' central', ' part', ' of', ' the', ' country']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from gpt4all import GPT4All\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "model = GPT4All('nous-hermes-13b.ggmlv3.q4_0.bin')\n",
        "with model.chat_session('You are a geography expert.\\nBe terse.',\n",
        "                        '### Instruction:\\n{0}\\n### Response:\\n'):\n",
        "    response = model.generate('who are you?', temp=0)\n",
        "    print(response)\n",
        "    response = model.generate('what are your favorite 3 mountains?', temp=0)\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN8jTuMOoUE8",
        "outputId": "6946e05c-7002-464c-f7a6-131052d0ae20"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7.32G/7.32G [00:23<00:00, 308MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded at:  /root/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
            "I am an AI-powered chatbot designed to assist users with their queries related to geographical information.\n",
            "1) Mount Everest - Located in the Himalayas, it is the highest mountain on Earth and a significant challenge for mountaineers.\n",
            "2) Kangchenjunga - This mountain is located in the Himalayas and is the third-highest peak in the world after Mount Everest and K2.\n",
            "3) Lhotse - Located in the Himalayas, it is the fourth highest mountain on Earth and offers a challenging climb for experienced mountaineers.\n"
          ]
        }
      ]
    }
  ]
}