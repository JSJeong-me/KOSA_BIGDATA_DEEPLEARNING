{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXSTRnkPeotw5I1/GQARdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/KOSA_BIGDATA_DEEPLEARNING/blob/main/GPT4All-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wame838rlxZY",
        "outputId": "1b97c985-985d-4e36-ab2b-f8e34ba0097b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-1.0.8-py3-none-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2023.7.22)\n",
            "Installing collected packages: gpt4all\n",
            "Successfully installed gpt4all-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install gpt4all"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
        "output = model.generate(\"The capital of France is \", max_tokens=1)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNisqxrpnDa9",
        "outputId": "cef38a1e-13ab-4240-ca1e-83f98982f890"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT4All(model_name='orca-mini-3b.ggmlv3.q4_0.bin')\n",
        "with model.chat_session():\n",
        "    response1 = model.generate(prompt='hello', temp=0)\n",
        "    response2 = model.generate(prompt='write me a short poem', temp=0)\n",
        "    response3 = model.generate(prompt='thank you', temp=0)\n",
        "    print(model.current_chat_session)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk0E9uG9m4As",
        "outputId": "530168b3-3979-4c70-b345-af866156507e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
            "[{'role': 'system', 'content': '### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': ' Hello! How may I assist you today?'}, {'role': 'user', 'content': 'write me a short poem'}, {'role': 'assistant', 'content': \" Sure, here's a short poem for you:\\nBeneath the autumn sky so blue,\\nThe leaves fall down like rain.\\nA rustling carpet of gold and red,\\nAs nature's beauty takes its fill.\\nThe air is crisp and cool and clean,\\nAnd the world around us grows.\"}, {'role': 'user', 'content': 'thank you'}, {'role': 'assistant', 'content': \" You're welcome! Is there anything else I can help you with?\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
        "tokens = []\n",
        "for token in model.generate(\"The capital of France is\", max_tokens=20, streaming=True):\n",
        "    tokens.append(token)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Ssx0d7mvaF",
        "outputId": "110c231a-6b1f-49a7-ea3d-1321a1ac7856"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
            "[' Paris', ',', ' is', ' the', ' largest', ' city', ' in', ' the', ' country', ' and', ' is', ' located', ' in', ' the', ' northern', ' central', ' part', ' of', ' the', ' country']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0NCrXFamLix",
        "outputId": "c0f80f40-1ad9-4252-a9de-6eba9ea2d14e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
            "[{'role': 'system', 'content': '### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': ' Hello! How may I assist you today?'}, {'role': 'user', 'content': 'write me a short poem'}, {'role': 'assistant', 'content': \" Sure, here's a short poem for you:\\nBeneath the autumn sky so blue,\\nThe leaves fall down like rain.\\nA rustling carpet of gold and red,\\nAs nature's beauty takes its fill.\\nThe air is crisp and cool and clean,\\nAnd the world around us grows.\"}, {'role': 'user', 'content': 'thank you'}, {'role': 'assistant', 'content': \" You're welcome! Is there anything else I can help you with?\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PYlupjuYmVke",
        "outputId": "72f07fc0-fe60-4fe7-d64b-ec4363ffc0b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}